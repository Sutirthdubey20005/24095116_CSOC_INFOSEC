CSOC InfoSec Forensics - Week 1, Day 7, Task 13: Proxy Log Analysis
Building Linux Command-Line Skills for Log Analysis
This repository contains the walkthrough and solutions for Task 13, Day 7 of the CSOC InfoSec Forensics program, focusing on developing essential Linux command-line skills for log analysis. The task involves manipulating and extracting relevant information from proxy logs using fundamental Linux commands such as cut, grep, sort, and uniq.

Objectives:
Understand the structure of proxy log entries.

Learn to use cut for field extraction.

Master grep for pattern searching.

Utilize sort for ordering and uniq for unique entry identification.

Apply these skills to analyze a proxy log dataset for suspicious activity.

Identify potentially malicious domains and data exfiltration attempts.

Analysing a Proxy Log Based on Typical Use Cases
The core of this task is a hands-on exercise to apply the acquired command-line skills to analyze a proxy log dataset. The analysis aims to hunt down potentially malicious activity, identify suspicious domains, and uncover potential data exfiltration attempts. This document not only covers the technical aspects of log analysis but also provides insights into the significance of each step in real-world cybersecurity scenarios.

WALKTHROUGH
Below are the questions posed during the task, along with the Linux commands used to find the answers and the corresponding results. The access.log file is assumed to be the proxy log dataset being analyzed.

Q#1: How many unique IP addresses are connected to the proxy server?
To determine the number of unique IP addresses, we extract the second field (IP address) from each log entry, sort them to group identical IPs, and then count the unique occurrences.

Command:

cut -d ' ' -f2 access.log | sort -u | wc -l

Explanation:

cut -d ' ' -f2 access.log: Extracts the second field from access.log, using a space as the delimiter. This field typically contains the source IP address.

sort -u: Sorts the extracted IP addresses and removes duplicate lines, leaving only unique IPs.

wc -l: Counts the number of lines (which now correspond to unique IP addresses).

Answer: 9

Q#2: How many unique domains were accessed by all workstations?
To find the number of unique domains, we extract the domain portion from the third field (URL/domain) of each log entry, sort them, and count the unique entries.

Command:

cut -d ' ' -f3 access.log | cut -d ':' -f1 | sort -u | wc -l

Explanation:

cut -d ' ' -f3 access.log: Extracts the third field (e.g., domain.com:port or http://domain.com/path).

cut -d ':' -f1: Further processes the output to extract only the domain part before the colon (if a port is present).

sort -u: Sorts the extracted domains and keeps only unique entries.

wc -l: Counts the number of unique domains.

Answer: 111

Q#3: What status code is generated by the HTTP requests to the least accessed domain?
This question requires identifying the domain with the fewest requests and then finding a status code associated with it. We first count the occurrences of each domain, sort them by count, and then examine the status code for the least frequent one.

Command:

cut -d ' ' -f3,6 access.log | sort | uniq -c | sort -n | head -n 1

Explanation:

cut -d ' ' -f3,6 access.log: Extracts the third field (domain) and the sixth field (status code).

sort: Sorts these combined fields to group identical domain-status code pairs.

uniq -c: Counts the occurrences of each unique domain-status code pair.

sort -n: Sorts the output numerically based on the count in ascending order.

head -n 1: Displays only the first line, which corresponds to the least accessed domain-status code pair.

Answer: 503 (This is the status code found for the least accessed domain based on the provided solution.)

Q#4: Based on the high count of connection attempts, what is the name of the suspicious domain?
To identify a suspicious domain based on a high count of connection attempts, we count the occurrences of each domain and sort them in descending order to find the most frequently accessed one.

Command:

cut -d ' ' -f3 access.log | sort | uniq -c | sort -nr

Explanation:

cut -d ' ' -f3 access.log: Extracts the third field (domain).

sort: Sorts the extracted domains.

uniq -c: Counts the occurrences of each unique domain.

sort -nr: Sorts the output numerically (-n) in reverse (-r) order, placing the most frequent domains at the top.

Answer: frostlings.bigbadstash.thm

Q#5: What is the source IP of the workstation that accessed the malicious domain?
Once the malicious domain is identified, we can grep for its occurrences in the log and then extract the source IP address associated with those entries.

Command:

grep "frostlings.bigbadstash.thm" access.log | cut -d ' ' -f2 | sort | uniq

Explanation:

grep "frostlings.bigbadstash.thm" access.log: Filters the access.log file to show only lines containing the malicious domain.

cut -d ' ' -f2: Extracts the second field (source IP address) from the filtered lines.

sort | uniq: Sorts the extracted IP addresses and then displays only the unique ones.

Answer: 10.10.185.225

Q#6: How many requests were made on the malicious domain in total?
To find the total number of requests to the malicious domain, we simply count the lines in the log file that contain the domain's name.

Command:

grep frostlings.bigbadstash.thm access.log | wc -l

Explanation:

grep frostlings.bigbadstash.thm access.log: Filters the access.log file for lines containing the malicious domain.

wc -l: Counts the number of lines (requests) that match the pattern.

Answer: 1581

Q#7: Having retrieved the exfiltrated data, what is the hidden flag?
This question implies that some data was exfiltrated via the malicious domain, likely encoded. We need to find the specific part of the log entry that contains this data (often after an equals sign in a URL parameter) and then decode it (e.g., using base64 -d).

Command:

grep "frostlings.bigbadstash.thm" access.log | cut -d ' ' -f5 | cut -d '=' -f2 | base64 -d

Explanation:

grep "frostlings.bigbadstash.thm" access.log: Filters for lines related to the malicious domain.

cut -d ' ' -f5: Extracts the fifth field, which is likely the URL or a parameter containing the encoded data.

cut -d '=' -f2: Further extracts the part of the field after the first equals sign, assuming the flag is a value in a key=value pair.

base64 -d: Decodes the extracted string from Base64 encoding.

Answer: THM{a_gift_for_you_awesome_analyst!}

This walkthrough demonstrates practical applications of b
